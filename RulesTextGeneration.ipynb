{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "cards_df = pd.read_csv('cards.csv')\n",
    "cards_df = cards_df.drop('name', axis=1)\n",
    "cards_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "cards_train, cards_val = train_test_split(cards_df, test_size=0.2)\n",
    "cards_train.to_csv('training.txt', index=False, header=False, sep=' ', quotechar='\"')\n",
    "cards_val.to_csv('val.txt', index=False, header=False, sep=' ', quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from io import StringIO\n",
    "\n",
    "class CardDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, content, tokenizer):\n",
    "        self.cards = []\n",
    "        if isinstance(content, str):\n",
    "            with open(content, \"r\", encoding=\"utf-8\") as file:\n",
    "                self.cards = list([line.strip() for line in file.readlines()])\n",
    "                file.close()\n",
    "        elif isinstance(content, pd.DataFrame):\n",
    "            self.cards = self.convert_df_to_strs(content)\n",
    "        else:\n",
    "            assert False, \"content is not a str or pd.DataFrame\"\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def convert_df_to_strs(self, df):\n",
    "        if 'name' in df.columns:\n",
    "            df = df.drop('name', axis=0)\n",
    "        buffer = StringIO()\n",
    "        df.to_csv(buffer, index=False, header=False, sep=\" \")\n",
    "        buffer.seek(0)\n",
    "        return [line.strip() for line in buffer.readlines()]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cards)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.cards[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tokens = [\"\\\"<card>\", \"</card>\\\"\", \"<line>\", \"<precolon>\", \"</precolon>\", \"<color>\", \"</color>\", \"<bullet>\"]\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents, Sequence\n",
    "\n",
    "bpe_tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "bpe_tokenizer.pre_tokenizer = Whitespace()\n",
    "bpe_tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "bpe_tokenizer.add_tokens(custom_tokens)\n",
    "\n",
    "bpe_tokenizer.train(['training.txt', 'val.txt'], min_frequency=2)\n",
    "\n",
    "vocab_file, merges_file = tuple(bpe_tokenizer.save_model('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4843109"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "from transformers import CTRLLMHeadModel, CTRLConfig\n",
    "  \n",
    "config = CTRLConfig(\n",
    "    vocab_size=bpe_tokenizer.get_vocab_size(), \n",
    "    n_positions=256,\n",
    "    n_ctx=256,\n",
    "    n_embd=256,\n",
    "    dff=128,\n",
    "    n_layer=12,\n",
    "    n_head=12\n",
    ")\n",
    "\n",
    "model = CTRLLMHeadModel(config)\n",
    "\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset\n",
    "train_dataset = CardDataset(cards_train, bpe_tokenizer)\n",
    "validation_dataset = CardDataset(cards_val, bpe_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=bpe_tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=1,   # batch size per device during training\n",
    "    per_device_eval_batch_size=1,    # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                     # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=args,                       # training arguments, defined above\n",
    "    data_collator=data_collator,     # data collator\n",
    "    train_dataset=train_dataset,     # training dataset\n",
    "    eval_dataset=validation_dataset  # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: buntry (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}